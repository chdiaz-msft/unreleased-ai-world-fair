name: Braintrust Evaluation

on:
  pull_request:
    branches: [main]
    types: [opened, synchronize, reopened]

jobs:
  eval:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install pnpm
        uses: pnpm/action-setup@v2
        with:
          version: latest

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'pnpm'

      - name: Install dependencies
        run: pnpm install --ignore-scripts

      - name: Push Braintrust functions
        env:
          BRAINTRUST_API_KEY: ${{ secrets.BRAINTRUST_API_KEY }}
        run: braintrust push braintrust --if-exists ignore

      - name: Run Braintrust evaluation
        id: eval
        env:
          BRAINTRUST_API_KEY: ${{ secrets.BRAINTRUST_API_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          # Run eval and capture all output
          pnpm eval 2>&1 | tee eval_output.txt
          
          # Extract key metrics (adjust regex patterns based on your actual output)
          EXPERIMENT_URL=$(grep -oP 'https://braintrust\.dev[^\s]*' eval_output.txt | head -1 || echo "")
          EXPERIMENT_ID=$(grep -oP 'Experiment.*?([a-f0-9-]{36})' eval_output.txt | tail -1 | grep -oP '[a-f0-9-]{36}' || echo "")
          
          # Extract scores (adjust based on your scorer names)
          ACCURACY=$(grep -oP 'changelog-accuracy-scorer.*?(\d+\.?\d*)' eval_output.txt | grep -oP '\d+\.?\d*' | tail -1 || echo "N/A")
          COMPLETENESS=$(grep -oP 'changelog-completeness-scorer.*?(\d+\.?\d*)' eval_output.txt | grep -oP '\d+\.?\d*' | tail -1 || echo "N/A")
          FORMATTING=$(grep -oP 'changelog-formatting-scorer.*?(\d+\.?\d*)' eval_output.txt | grep -oP '\d+\.?\d*' | tail -1 || echo "N/A")
          
          # Count test cases
          TOTAL_CASES=$(grep -c "input" eval_output.txt || echo "0")
          
          # Check for failures or errors
          if grep -qi "error\|failed\|exception" eval_output.txt; then
            echo "eval_status=failed" >> $GITHUB_OUTPUT
            FAILURE_REASON=$(grep -i "error\|failed\|exception" eval_output.txt | head -3 | tr '\n' ' ')
            echo "failure_reason=$FAILURE_REASON" >> $GITHUB_OUTPUT
          else
            echo "eval_status=passed" >> $GITHUB_OUTPUT
          fi
          
          # Set outputs
          echo "experiment_url=$EXPERIMENT_URL" >> $GITHUB_OUTPUT
          echo "experiment_id=$EXPERIMENT_ID" >> $GITHUB_OUTPUT
          echo "accuracy_score=$ACCURACY" >> $GITHUB_OUTPUT
          echo "completeness_score=$COMPLETENESS" >> $GITHUB_OUTPUT
          echo "formatting_score=$FORMATTING" >> $GITHUB_OUTPUT
          echo "total_cases=$TOTAL_CASES" >> $GITHUB_OUTPUT
          
          # Set thresholds (adjust these values)
          ACCURACY_THRESHOLD=0.8
          COMPLETENESS_THRESHOLD=0.85
          FORMATTING_THRESHOLD=0.9
          
          # Check if scores meet thresholds
          if (( $(echo "$ACCURACY < $ACCURACY_THRESHOLD" | bc -l) )) && [ "$ACCURACY" != "N/A" ]; then
            echo "threshold_failures=Accuracy below threshold ($ACCURACY < $ACCURACY_THRESHOLD)" >> $GITHUB_OUTPUT
            echo "eval_status=failed" >> $GITHUB_OUTPUT
          elif (( $(echo "$COMPLETENESS < $COMPLETENESS_THRESHOLD" | bc -l) )) && [ "$COMPLETENESS" != "N/A" ]; then
            echo "threshold_failures=Completeness below threshold ($COMPLETENESS < $COMPLETENESS_THRESHOLD)" >> $GITHUB_OUTPUT
            echo "eval_status=failed" >> $GITHUB_OUTPUT
          elif (( $(echo "$FORMATTING < $FORMATTING_THRESHOLD" | bc -l) )) && [ "$FORMATTING" != "N/A" ]; then
            echo "threshold_failures=Formatting below threshold ($FORMATTING < $FORMATTING_THRESHOLD)" >> $GITHUB_OUTPUT
            echo "eval_status=failed" >> $GITHUB_OUTPUT
          fi

      - name: Upload eval output as artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: eval-output
          path: eval_output.txt
          retention-days: 30

      - name: Comment PR with detailed eval results
        if: always()
        uses: actions/github-script@v7
        with:
          script: |
            const evalStatus = '${{ steps.eval.outputs.eval_status }}';
            const experimentUrl = '${{ steps.eval.outputs.experiment_url }}';
            const experimentId = '${{ steps.eval.outputs.experiment_id }}';
            const accuracyScore = '${{ steps.eval.outputs.accuracy_score }}';
            const completenessScore = '${{ steps.eval.outputs.completeness_score }}';
            const formattingScore = '${{ steps.eval.outputs.formatting_score }}';
            const totalCases = '${{ steps.eval.outputs.total_cases }}';
            const thresholdFailures = '${{ steps.eval.outputs.threshold_failures }}';
            const failureReason = '${{ steps.eval.outputs.failure_reason }}';
            
            const statusEmoji = evalStatus === 'passed' ? 'âœ…' : 'âŒ';
            const statusText = evalStatus === 'passed' ? 'PASSED' : 'FAILED';
            
            let body = `## ğŸ§  Braintrust Evaluation Results\n\n`;
            body += `**Status:** ${statusEmoji} ${statusText}\n`;
            body += `**Test Cases:** ${totalCases}\n`;
            if (experimentId) {
              body += `**Experiment ID:** \`${experimentId}\`\n`;
            }
            body += `\n### ğŸ“Š Scores\n\n`;
            body += `| Metric | Score | Status |\n`;
            body += `|--------|-------|--------|\n`;
            
            const formatScore = (score, threshold) => {
              if (score === 'N/A') return 'N/A';
              const numScore = parseFloat(score);
              const emoji = numScore >= threshold ? 'âœ…' : 'âŒ';
              return `${score} ${emoji}`;
            };
            
            body += `| Accuracy | ${formatScore(accuracyScore, 0.8)} | Threshold: â‰¥0.8 |\n`;
            body += `| Completeness | ${formatScore(completenessScore, 0.85)} | Threshold: â‰¥0.85 |\n`;
            body += `| Formatting | ${formatScore(formattingScore, 0.9)} | Threshold: â‰¥0.9 |\n`;
            
            if (experimentUrl) {
              body += `\nğŸ“ˆ **[View detailed results â†’](${experimentUrl})**\n`;
            } else {
              body += `\nğŸ“ˆ **[View results in Braintrust dashboard â†’](https://braintrust.dev)**\n`;
            }
            
            if (evalStatus === 'failed') {
              body += `\n### âš ï¸ Failure Details\n\n`;
              if (thresholdFailures) {
                body += `**Threshold violations:** ${thresholdFailures}\n`;
              }
              if (failureReason) {
                body += `**Error details:** \`${failureReason}\`\n`;
              }
              body += `\nğŸš« **This PR cannot be merged until evaluations pass.**`;
            } else {
              body += `\nâœ¨ **All evaluations passed! This PR is ready for review.**`;
            }
            
            // Add artifact link
            body += `\n\nğŸ” [Download full eval output](https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: body
            });

      - name: Fail job if eval failed
        if: steps.eval.outputs.eval_status == 'failed'
        run: |
          echo "âŒ Evaluation failed - check the results above"
          exit 1

      - name: Summary
        if: steps.eval.outputs.eval_status == 'passed'
        run: |
          echo "âœ… All evaluations passed!"
          echo "ğŸ“Š Accuracy: ${{ steps.eval.outputs.accuracy_score }}"
          echo "ğŸ“Š Completeness: ${{ steps.eval.outputs.completeness_score }}"
          echo "ğŸ“Š Formatting: ${{ steps.eval.outputs.formatting_score }}"