name: Braintrust Evaluation

on:
  pull_request:
    branches: [main]
    types: [opened, synchronize, reopened]

jobs:
  eval:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install pnpm
        uses: pnpm/action-setup@v2
        with:
          version: latest

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'pnpm'

      - name: Install dependencies
        run: pnpm install --ignore-scripts

      - name: Push Braintrust functions
        env:
          BRAINTRUST_API_KEY: ${{ secrets.BRAINTRUST_API_KEY }}
        run: npx braintrust push braintrust --if-exists ignore

      - name: Run Braintrust evaluation
        id: eval
        env:
          BRAINTRUST_API_KEY: ${{ secrets.BRAINTRUST_API_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          # Run eval and capture all output
          pnpm eval 2>&1 | tee eval_output.txt
          
          # Extract experiment URL - looking for the actual URL pattern from your output
          EXPERIMENT_URL=$(grep -o 'https://[^ ]*braintrust[^ ]*' eval_output.txt | head -1 || echo "")
          
          # Extract experiment ID from the URL or output
          EXPERIMENT_ID=$(echo "$EXPERIMENT_URL" | grep -o '[A-Z]*-[0-9]*' | head -1 || echo "")
          
          # Extract scores - based on your actual output format
          ACCURACY=$(grep "Changelog Accuracy Scorer" eval_output.txt | grep -o '[0-9]*\.[0-9]*%' | grep -o '[0-9]*\.[0-9]*' | head -1 || echo "N/A")
          COMPLETENESS=$(grep "Changelog Completeness" eval_output.txt | grep -o '[0-9]*\.[0-9]*%' | grep -o '[0-9]*\.[0-9]*' | head -1 || echo "N/A")  
          FORMATTING=$(grep "changelog-formatting-scorer" eval_output.txt | grep -o '[0-9]*\.[0-9]*%' | grep -o '[0-9]*\.[0-9]*' | head -1 || echo "N/A")
          
          # Count test cases - safer approach
          TOTAL_CASES=$(grep -c "datapoints" eval_output.txt || echo "unknown")
          if [ "$TOTAL_CASES" = "0" ]; then
            TOTAL_CASES="unknown"
          fi
          
          # Check for failures or errors - properly escape all special characters
          if grep -qi "error\|failed\|exception\|unable to process" eval_output.txt; then
            echo "eval_status=failed" >> $GITHUB_OUTPUT
            # Use base64 encoding to safely pass the error message
            FAILURE_REASON=$(grep -i "error\|failed\|exception\|unable to process" eval_output.txt | head -1 | base64 -w 0)
            echo "failure_reason_encoded=${FAILURE_REASON}" >> $GITHUB_OUTPUT
          else
            echo "eval_status=passed" >> $GITHUB_OUTPUT
          fi
          
          # Set outputs with proper escaping
          echo "experiment_url=${EXPERIMENT_URL}" >> $GITHUB_OUTPUT
          echo "experiment_id=${EXPERIMENT_ID}" >> $GITHUB_OUTPUT
          echo "accuracy_score=${ACCURACY}" >> $GITHUB_OUTPUT
          echo "completeness_score=${COMPLETENESS}" >> $GITHUB_OUTPUT
          echo "formatting_score=${FORMATTING}" >> $GITHUB_OUTPUT
          echo "total_cases=${TOTAL_CASES}" >> $GITHUB_OUTPUT
          
          # Convert percentages to decimals for threshold checking
          if [ "$ACCURACY" != "N/A" ]; then
            ACCURACY_DECIMAL=$(echo "$ACCURACY" | awk '{print $1/100}')
          else
            ACCURACY_DECIMAL="N/A"
          fi
          
          if [ "$COMPLETENESS" != "N/A" ]; then
            COMPLETENESS_DECIMAL=$(echo "$COMPLETENESS" | awk '{print $1/100}')
          else
            COMPLETENESS_DECIMAL="N/A"  
          fi
          
          if [ "$FORMATTING" != "N/A" ]; then
            FORMATTING_DECIMAL=$(echo "$FORMATTING" | awk '{print $1/100}')
          else
            FORMATTING_DECIMAL="N/A"
          fi
          
          # Set thresholds (as decimals to match converted scores)
          ACCURACY_THRESHOLD=0.8
          COMPLETENESS_THRESHOLD=0.85
          FORMATTING_THRESHOLD=0.9
          
          # Check thresholds
          THRESHOLD_FAILURES=""
          if [ "$ACCURACY_DECIMAL" != "N/A" ] && [ "$(echo "$ACCURACY_DECIMAL $ACCURACY_THRESHOLD" | awk '{print ($1 < $2)}')" = "1" ]; then
            THRESHOLD_FAILURES="Accuracy below threshold (${ACCURACY}% < 80%)"
            echo "eval_status=failed" >> $GITHUB_OUTPUT
          elif [ "$COMPLETENESS_DECIMAL" != "N/A" ] && [ "$(echo "$COMPLETENESS_DECIMAL $COMPLETENESS_THRESHOLD" | awk '{print ($1 < $2)}')" = "1" ]; then
            THRESHOLD_FAILURES="Completeness below threshold (${COMPLETENESS}% < 85%)"
            echo "eval_status=failed" >> $GITHUB_OUTPUT
          elif [ "$FORMATTING_DECIMAL" != "N/A" ] && [ "$(echo "$FORMATTING_DECIMAL $FORMATTING_THRESHOLD" | awk '{print ($1 < $2)}')" = "1" ]; then
            THRESHOLD_FAILURES="Formatting below threshold (${FORMATTING}% < 90%)"
            echo "eval_status=failed" >> $GITHUB_OUTPUT
          fi
          
          echo "threshold_failures=${THRESHOLD_FAILURES}" >> $GITHUB_OUTPUT

      - name: Upload eval output as artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: eval-output
          path: eval_output.txt
          retention-days: 30

      - name: Comment PR with detailed eval results
        if: always()
        uses: actions/github-script@v7
        with:
          script: |
            const evalStatus = '${{ steps.eval.outputs.eval_status }}' || 'unknown';
            const experimentUrl = '${{ steps.eval.outputs.experiment_url }}' || '';
            const experimentId = '${{ steps.eval.outputs.experiment_id }}' || '';
            const accuracyScore = '${{ steps.eval.outputs.accuracy_score }}' || 'N/A';
            const completenessScore = '${{ steps.eval.outputs.completeness_score }}' || 'N/A';
            const formattingScore = '${{ steps.eval.outputs.formatting_score }}' || 'N/A';
            const totalCases = '${{ steps.eval.outputs.total_cases }}' || 'unknown';
            const thresholdFailures = '${{ steps.eval.outputs.threshold_failures }}' || '';
            
            // Decode the base64 encoded error message safely
            let failureReason = '';
            const encodedFailureReason = '${{ steps.eval.outputs.failure_reason_encoded }}' || '';
            if (encodedFailureReason) {
              try {
                failureReason = Buffer.from(encodedFailureReason, 'base64').toString('utf-8');
                // Clean up the message and limit length
                failureReason = failureReason.replace(/[`'"]/g, '').substring(0, 200);
              } catch (e) {
                failureReason = 'Error message could not be decoded';
              }
            }
            
            const statusEmoji = evalStatus === 'passed' ? '‚úÖ' : '‚ùå';
            const statusText = evalStatus === 'passed' ? 'PASSED' : 'FAILED';
            
            let body = '## üß† Braintrust Evaluation Results\n\n';
            body += `**Status:** ${statusEmoji} ${statusText}\n`;
            body += `**Test Cases:** ${totalCases}\n`;
            if (experimentId) {
              body += `**Experiment ID:** \`${experimentId}\`\n`;
            }
            body += '\n### üìä Scores\n\n';
            body += '| Metric | Score | Status |\n';
            body += '|--------|-------|--------|\n';
            
            const formatScore = (score, threshold) => {
              if (score === 'N/A') return 'N/A';
              // Handle percentage format
              const numScore = parseFloat(score);
              const emoji = numScore >= (threshold * 100) ? '‚úÖ' : '‚ùå';
              return `${score}% ${emoji}`;
            };
            
            body += `| Accuracy | ${formatScore(accuracyScore, 0.8)} | Threshold: ‚â•80% |\n`;
            body += `| Completeness | ${formatScore(completenessScore, 0.85)} | Threshold: ‚â•85% |\n`;
            body += `| Formatting | ${formatScore(formattingScore, 0.9)} | Threshold: ‚â•90% |\n`;
            
            if (experimentUrl) {
              body += `\nüìà **[View detailed results ‚Üí](${experimentUrl})**\n`;
            } else {
              body += '\nüìà **[View results in Braintrust dashboard ‚Üí](https://braintrust.dev)**\n';
            }
            
            if (evalStatus === 'failed') {
              body += '\n### ‚ö†Ô∏è Failure Details\n\n';
              if (thresholdFailures) {
                body += `**Threshold violations:** ${thresholdFailures}\n`;
              }
              if (failureReason) {
                body += `**Error details:** ${failureReason}\n`;
              }
              body += '\nüö´ **This PR cannot be merged until evaluations pass.**';
            } else {
              body += '\n‚ú® **All evaluations passed! This PR is ready for review.**';
            }
            
            body += `\n\nüîç [Download full eval output](https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})`;
            
            await github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: body
            });

      - name: Fail job if eval failed
        if: steps.eval.outputs.eval_status == 'failed'
        run: |
          echo "‚ùå Evaluation failed - check the results above"
          exit 1

      - name: Summary
        if: steps.eval.outputs.eval_status == 'passed'
        run: |
          echo "‚úÖ All evaluations passed!"
          echo "üìä Accuracy: ${{ steps.eval.outputs.accuracy_score }}%"
          echo "üìä Completeness: ${{ steps.eval.outputs.completeness_score }}%"
          echo "üìä Formatting: ${{ steps.eval.outputs.formatting_score }}%"